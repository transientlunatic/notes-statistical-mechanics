
\section{Definitions}
\label{sec:definitions}

\subsection{States}
\label{sec:states}

\begin{definition}[Macrostate]
  A macrostate is a description of a system which can be described by
  a few parameters, for example, temperature, pressure, internal
  energy, and volume.
\end{definition}

\begin{definition}[Microstate]
  A microstate is a detailed description of the constituent components of a macrostate, e.g.
  \begin{itemize}
  \item Classically, the position and momentum of each particle in the state;
  \item Quantum mechanically, the state vector for each particle.
  \end{itemize}
\end{definition}

\begin{postulate}
  \label{pos:averageover}
  We can identify the macroscopic parameters with the average at a
  given time over the set ensemble of microstates compatible with the
  macroscopic state and the environment of the system.
\end{postulate}
\begin{postulate}
  \label{pos:equalrep}
  For a given set of macroscopic parameters all quantum states
  consistent with this set have equal representation (i.e. weighting
  or probability) in the microcanonical ensemble.
\end{postulate}
\begin{postulate}[Averaging Postulate]
  All accessible microstates are equally probable.
\end{postulate}

\subsection{Ensembles}
\label{sec:ensembles}

\begin{definition}[Microcanonical Ensemble]
  The microcanonical ensemble represents the possible microstates of a
  mechanical system which have the same fixed total energy, and
  particle number.  All elements have the same volume, particle
  number, and total energy.
\end{definition}

\begin{definition}[Canonical Ensemble]
  This ensemble describes a closed system which is in contact with a
  heat bath. The total energy of each microstate is not constrained to
  be identical, with each system weighted by the Boltzmann factor. The
  volume, particle number, and temperature are fixed.
\end{definition}

\begin{definition}[Grand Canonical Ensemble]
  In this ensemble neither energy nor particle number are fixed, so
  temperature and chemical potential must be specified. This describes
  an open system.
\end{definition}

\subsection{Steps to model a system}
\label{sec:steps-model-system}

Problems in statistical modelling can be handled in broadly the same
way.
\begin{enumerate}
\item Solve the one-particle problem
\item Enumerate all of the possible distributions
\item Count the number of microstates which are in each distribution
\item Find the average distribution
\end{enumerate}

\subsection{Examples of Microcanonical Distributions}
\label{sec:exampl-micr-distr}

\begin{example}[Spin-1 Particle]
  Consider a single spin-1 particle in a magnetic field
  $\vec{B}$. There are three possible states, which have energies $E =
  \set{- \gamma m B}$ for $m=\set{-1, 0, 1}$.\\
  If $B=0$ then $E~{m}=0$ for all $m$, and so there is equal
  probability of being in any $m$ state, thus the probability of each
  state is $P = 1/3$. 
  If $\vec{B} \neq 0$ we cannot assign probabilities without a
  constraint;
  total energy, so the state is not microcanonical.\\
  If $E=0$ for $\vec{B}$ the system must have $m=0$ with $P=1$.
\end{example}
\begin{example}[Two spin-1 particles]
  Consider two spin-1 particles, $x$ and $y$ which are
  distinguishable. What is the probability of $m_x=+1$ while $m_y=-1$,
  i.e.~that the system has a state vector $(+1, -1)$?

  In the case that $B=0$ the state $(+1, -1)$ is one of 9 possible
  states, and so $P=\frac{1}{9}$. If, however, $B\neq 0$, we need to
  know $E$, so again, the state is non-microcanonical. If $E=0$ the
  states can be $(+1, -1)$, $(-1, +1)$, or $(0,0)$, so
  $P=\frac{1}{3}$.
\end{example}

\begin{example}[One dimensional polymer molecule]
  Consider a molecule consisting of $A$ links, each of length
  $b$. Each link may have one of two directions, denoted $\rightarrow$
  and $\leftarrow$. Each orientation has the same energy, so the
  ensemble is microcanonical, and this is an ideal microcanonical
  system.

  Let one end of the polymer be at $x=0$ and the other at $x = Lb$,
  for $-A \leq L \leq A$. Recalling postulate \ref{pos:averageover},
  the macroscopic length will be the average over all of the
  ensemble. From postulate \ref{pos:equalrep} we know that all of the
  $2^A$ microstates are equally probable, but this does not imply all
  macrostates are equally probable.

  A system of three links has $2^3$ possible configurations, so

  \begin{tabular}{cccc}
                                     & \rightarrow \leftarrow \leftarrow  & \leftarrow \rightarrow \rightarrow &                                     \\
    \leftarrow \leftarrow \leftarrow & \leftarrow \rightarrow \rightarrow & \rightarrow \leftarrow \rightarrow & \rightarrow \rightarrow \rightarrow \\ 
                                     & \leftarrow \leftarrow \rightarrow  & \rightarrow \rightarrow \leftarrow &                                     \\
$L = -3$                             & $L = -1$                           & $L =+1$                            & $L=+3$                              \\
$P=\frac{1}{8}$                      & $P=\frac{3}{8}$                    & $P=\frac{3}{8}$                    & $P=\frac{1}{8}$
  \end{tabular}

  Clearly there are a preferred set of orientations giving a length
  $|L|=1$ for the macroparameter. We can generalise this result by
  applying the binomial theorem.

  The total number of configurations with a fixed end at $x = Lb$ is
  given by a binomial distribution (see appendix
  \ref{sec:binom-distr}) such that
  \begin{equation}
    \label{eq:7}
    \Omega(L) = \frac{A!}{a_+! a_-!}
  \end{equation}
  where $a_+$ is the number of links pointing to the right, and $a_-$
  the number pointing to the left, such that 
  \[ a_+ + a_- = A, \quad a_+ - a_- = L \]
  Now,
  \begin{equation}
    \label{eq:8}
    \Omega(L) = \frac{A!}{\qty(\frac{A+L}{2})! \qty(\frac{A-L}{2})!}
  \end{equation}
  Then, taking Stirling's approximation (see appendix
  \ref{sec:stirl-appr}) 
  \begin{equation}
    \label{eq:9}
    P\qty(\frac{L}{A}) = \frac{\Omega(L)}{2^A} = \qty(\frac{2}{\pi a})^{\half} \exp( - \frac{L^2}{2A} )
  \end{equation}
  which has the form of a Gaussian distribution (which follows by the
  Central Limit Theorem). This indicates a most probable length of
  $L=0$.
\end{example}

\section{Canonical Distributions}
\label{sec:canon-distr}

% The microcanonical distribution affords great insight into physical
% systems but has limited direct relevance or application. We now
% introduce temperature, and remove the requirement that microstates all
% have the same energy. This allows the construction of a canonical
% ensemble from assemblies of microcanonical systems.  The system is
% controlled by contact with a heat bath at a temperature $T$, and all
% microstate energies, $E_i$ must be positive, so how are they
% distributed in probbaility?

In a microcanonical ensemble the energy of the state takes a
$\delta$-function form, but in a canonical ensemble, the energy of
each state is not the same---there is a distribution; we only know the
total energy of the system, which is constrained by the heat bath.

Although the microcanonical ensemble can be very useful it doesn't
occur often in real physical systems. A better approximation is
obtained by considering systems with a fixed number of particles,
volume, and temperature, held in a heat bath which defines the
temperature, $T$. The system is isolated, as the heat bath is
impermeable to particles, but energy is transferred to maintain the
temperature. (e.g. the average mark in a distribution of test results
being fixed; individuals can have a range of marks not equal to the
average, thus the systems within the canonical ensemble have an energy
constrained only by the average.)

We can build insight into a canonical system by building it from
smaller microcanonical systems which contribute overall to the
measurables. To see this, consider a system of $A$ identical
sub-systems sharing a total energy $E~{tot}$. Let $E_i$ denote the
energy of the $i$-th state. If $a_i$ is the number of systems at any
time $t$ with energy $E_i$ then the set of numbers $\set{a_i}$ satisfies 
\[ \sum_i a_i = A \]
and
\[ \sum_i a_i E_i = E~{tot} = AU = A \bar{E} \] for $\bar{E} = U$ the
average energy of the sub-systems.

Any set of $\set{a_i}$ satisfying these constraints represents a
possible mode of the distribution of total energy $E~{tot}$ among $A$
members of the ensemble. Any set $\set{a_i}$ satisfying the
constraints can be realised in a number of ways, e.g.~A reshuffle
among those members of the ensemble with different energy values, and
thus obtain a state of the ensemble which is distinct from the
original. How many ways are there to do this?

Let $\Omega$ be the number of ways that a set can be arranged, then
\begin{equation}
  \label{eq:1}
  \Omega(\set{a_i}) = \frac{A!}{a_1! a_2! a_3! \cdots} = \frac{A!}{\prod_i a_i!}
\end{equation}

Since all possible states of the ensemble are equally likely to occur
the frequency with which the distribution $\set{a_i}$ appears is
directly in proportion to $\Omega(\set{a_i})$. Thus, the most probable
mode of distribution is the one maximising $\Omega(\set{a_i})$, which
we denote $\set{a_i^{*}}$. This clearly satisfies the constraints, and
for all proactical purposes it's the only one which we need to
consider.

For large $A$ we expect $\Omega$ will be very strongly peaked, so
let's maximise $\Omega$, or, as it happens, maximise
$\frac{\log(\Omega)}{A}$, and define
\[ H = \frac{\log(\Omega)}{A} \]
We maximise $H$ subject to the constraints
\begin{subequations}
\begin{align}
  \sum a_i &= A \\
\sum a_i E_i &= E~{tot}
\end{align}
\end{subequations}

\begin{align*}
  H = \frac{\log(\Omega)}{A} &= \frac{1}{A} \log( \frac{A!}{a_1! a_2! \cdots}) \\
&= \frac{1}{A} \qty[ \log(A!) - \log(a_1! a_2! \cdots)]\\
&= \frac{1}{A} \qty[ A \log(A) - A - \floor{\sum_i a_i \log(a_i) - a_i}]
\end{align*}
Now we define the probability of being in state $a_i$ as 
\[ P_i = \frac{a_i}{A} \]
thus $\sum P_i = 1$.

So 
\begin{align*}
  A &= \frac{1}{A} \qty[ A \log(A) - A - \qty{ \sum_i A P_i \log(A P_i) - A P_i}] \\
&= \frac{1}{A} \qty[ A \log(A) - A - A \qty{ \sum_i P_i \qty[\log(A) + \log(P_i)] - P_i}]
\end{align*}
Cancellations mean that 
\[ H = - \sum P_i \log(P_i) \]
which needs to be maximised.

Let $\alpha$, $\beta$ be Lagrange multipliers, and 
\[ f = - \sum_i P_i \log(P_i) + \alpha(1 - \sum_i P_i) + \beta( u - \sum_i P_i E_i )\]
We then form the differential,
\[ \dd{f} = \sum_i \set{ - \log(P_i) - 1 - \alpha - \beta E_i}
\dd{P_i} = 0 \] This must hold for all values of $i$, so we can set
each side to equal $0$ independently,
\[ \therefore - \log(P_i) - 1 -\alpha - \beta E_i =0 \quad \forall i \]
\[ P_i = \exp( -1 -\alpha -\beta E_i) \]
and we also know $\sum P_i=1$, so
\begin{align*}
  \sum \exp(-1 -\alpha - \beta E_i) &= 1 \\
  e^{-(1+\alpha)} \sum e^{-\beta E_i} &= 1 \\
  e^{-(1+\alpha)} =  \qty(\sum e^{-\beta E_i})^{-1} &= \frac{1}{Z} 
\end{align*}
where $Z = \sum e^{-\beta E_i}$ is the partition function for the
system, the sum over all states weighted by the Boltzmann factor. Thus
\begin{equation}
  \label{eq:10}
  P_i = \frac{1}{Z} \exp(-\beta E_i)
\end{equation}
This can be generalised to reflect the fact that there are several
ways to reach the same energy state, and so we adopt the notation
\begin{equation}
  \label{eq:11}
  Z = \sum_i g_i \exp(- \beta E_i)
\end{equation}
for $g_i$ the multiplicity (or degeneracy) of the $i$th state.

The partition function, $Z$, is the central equation of statistical
mechanics, and knowledge of it allows the derivation of the major
results of thermodynamics.

\subsection{Major results using $Z$}
\label{sec:major-results-using}

\emph{The mean energy in a canonical ensemble} is given as
\[ \ev{E} = \sum P_i E_i = \sum \frac{1}{Z} E_i \exp(-\beta E_i) \]
Considering that 
\[ \pdv{\beta} \log(Z) = \frac{1}{Z} \pdv{\beta}(Z) \]
and 
\[ Z = Z = \sum_i \exp(- \beta E_i) \]
then
\[ \pdv{\beta} \log(Z) = - \frac{1}{Z} \sum_i E_i \exp(- \beta E_i) \]
and so
\begin{equation}
\label{eq:12}
 U = \ev{E} = - \pdv{\beta} \log(Z) 
\end{equation}

\emph{The energy fluctuations in a canonical ensemble} are
\begin{align*} 
\Delta E^2 &= \ev{E_i - \ev{E}}^2 = \ev{E_i^2 - 2 E_i \ev{E} + \ev{E}^2} \\
 &= \ev{E_i}^2 - \ev{E}^2 = \sum_i P_i E_i^2 - \qty( \sum_i P_i E_i )^2 \\
 &= \sum_i \frac{1}{Z} \exp(-\beta E_i) E_i^2 - \qty( \sum_i \frac{1}{Z} \exp(- \beta E_i) E_i )^2 \tag{\(\star\)}
\end{align*}
Noting that 
\begin{align*}
  - \pdv{U}{\beta} &= - \pdv{\beta}( \pdv{Z}{\beta} \frac{1}{Z} ) \\ 
&= \pdv{\beta} \qty[ \qty(\sum_i e^{-\beta E_i})^{-1} \sum_i \qty(-E_i e^{\beta E_i})] \\
&= \star
\end{align*}
then
\begin{equation}
  \label{eq:13}
  \Delta E^2 = - \pdv{U}{\beta}
\end{equation}
which is positive definite.

% \newpage

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../project"
%%% End: 
